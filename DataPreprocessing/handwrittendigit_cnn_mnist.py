# -*- coding: utf-8 -*-
"""handwrittenDigit_CNN_MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1imInf3s1nN6LTTbou2cLwhLA9Zb0d-xK
"""

# import various packages which will be used
# import numpy as np
# import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import torchvision
from torchvision import datasets, transforms
from torch.utils.data import DataLoader


import matplotlib.pyplot as plt

#Used to check if GPU is available and set the device accordingly which will be used to train and test the model
#making sure that the model and data is on the same device to ensure correct and efficient execution
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#module withing torchvision - preprocessing and augmentation - resizing, converting to tensor, normalizing
#mostly used with vision dataset
transform = transforms.Compose([transforms.ToTensor()])

#uses torchvision library to download mnist dataset.
#root: is used to specify the directory where the data should be downloaded.
#train: specifies is its training(TRUE) or test(FALSE) data.
#download: checks for the dataset in the root and if not found allows download.
#transform: the series of transformation that are to be applied to the data as they are loaded.

mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# DataLoader class enable efficient handling of data during training and testing
# Can perform batching, shuffling, parallel loading
train_dataloader = DataLoader(mnist_trainset,batch_size = 64, shuffle=True)
test_dataloader = DataLoader(mnist_testset,batch_size = 64, shuffle=False)

# Lets explore the train and the test dataset
# Display image and label.

train_features, train_labels = next(iter(train_dataloader))
# iter(train_dataloader) => creates an iterator for our dataloader object
# next(...) => returns the next item in the iterator ; since its the first time we are calling next, it will return the first element

#train_features and train_labels are both torch.tensor type

print(f"\n Feature batch shape: {train_features.size()}")
#torch.Size([64, 1, 28, 28])
#  -- 64 is the batch size => each iteration (training loop) will process 64 images at once
#  -- 1 is the number of channels => 1 for grayscale, 3 for RGB images ; Should match with number of filters
#  -- 28 height of the image in pixel
#  -- 28 width of the image in pixel

print(f"\n Labels batch shape: {train_labels.size()}")
#torch.Size([64])
#similar to above it will return the size of the train label object

# Lets see one of the data points - image as well as the label in the training set
img = train_features[0].squeeze() #squeeze helps us remove all demensions where size is equal to 1 ; if none are found then the tensor is returned as is
label = train_labels[0]
plt.imshow(img, cmap="gray") #cmap refers to color map ; if gray scale is not specified for gray scale images then it will pick up some default colors.
plt.show()
print(f"Label: {label}")

# lets define the architecture of the neural network
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x)

#lets define some of the parameters

#tells us the number of times entire training dataset is passed through the model being trained;
#as the model goes through more epochs it gradually improves its performance
#underfit => train with too little epochs
#overfit => train with too many epochs
#early stopping => training until you observe decrease in accuracy on validation set
num_epochs = 3

#number of samples that are processed at once
batch_size_train = 64
batch_size_test = 1000


# random_seed = 1
# torch.backends.cudnn.enabled = False
# torch.manual_seed(random_seed)

#create a model object known as network
network = Net()

#hyperparameter of the optimizer ; the step size taken to adjust the weights ; weight used for the updates
learning_rate = 0.01

#hyperparameter for the optimizer; controls how much of the past updates need to be considered
momentum = 0.5

#creates a stochastic gradient descent object
optimizer = optim.SGD(network.parameters(), lr=learning_rate,
                      momentum=momentum)

# this parameter helps control the frequency of logging
log_interval = 10



train_losses = []
train_counter = []
# test_losses = []
test_counter = [i*len(train_dataloader.dataset) for i in range(num_epochs + 1)]

# optimizer = optim.Adam(model.parameters(), lr=0.001)
# loss_fn = nn.CrossEntropyLoss()

#This is the training function
def train(epoch):
  #lets the model know that the data being provided is for training because model behaviour is different while training and testing
  #for instance drop out layers randomly deactivate during training, batch norm layers
  network.train()

  #for loop for the training; it picks up one batch at a time from the dataloader object
  for batch_idx, (data, target) in enumerate(train_dataloader):
    #it resets gradient from the previous batch before calculating for the next batch
    optimizer.zero_grad()

    #stores the prediction made by the model using the current set of weight for all data points in data object
    output = network(data)

    #this computed the loss based on the predictions ;
    #negative log likelihood usually used for multi class classification where model output log probabilities
    loss = F.nll_loss(output, target)

    #performs a backpropagration pass based on the loss which is computed
    # that is it computes the gradients
    loss.backward()

    #performs optimization based on the gradients computed in the step above
    optimizer.step()

    #based on the batchid we display the results of each batch so as to not overwhelm the user
    if batch_idx % log_interval == 0:
      print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
        epoch, batch_idx * len(data), len(train_dataloader.dataset),
        100. * batch_idx / len(train_dataloader), loss.item()))
      train_losses.append(loss.item())
      train_counter.append(
        (batch_idx*64) + ((epoch-1)*len(train_dataloader.dataset)))

#this is the function used for testing
def test():

  #sets the model into evaluation mode
  network.eval()

  #sets the test loss to be zero
  test_loss = 0
  correct = 0


  with torch.no_grad(): #makes sure no backward pass and hence no gradients updated
    for data, target in test_dataloader:   #for loop for the testing; it picks up one batch at a time from the dataloader object
      output = network(data)    ##stores the prediction made by the model using the current set of weight for all data points in data object
      test_loss += F.nll_loss(output, target, size_average=False).item() #computes the loss and adds it to the loss variable
      pred = output.data.max(1, keepdim=True)[1]
      correct += pred.eq(target.data.view_as(pred)).sum()
  test_loss /= len(test_dataloader.dataset) #computes the average test loss based on the number of batches
  # test_losses.append(test_loss)

  #prints accuracy and loss
  print('\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
    test_loss, correct, len(test_dataloader.dataset),
    100. * correct / len(test_dataloader.dataset)))

test()
for epoch in range(1, num_epochs + 1):
  train(epoch)
  test()













